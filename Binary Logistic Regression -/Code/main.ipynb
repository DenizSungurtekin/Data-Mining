{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP4: Logistic Regression - L2 Regularizer - GD - Newton\n",
    "\n",
    "\n",
    "In this TP you have to implement two classifiers, the logistic who classify between two classes and the softmax who extends to many classes.\n",
    "\n",
    "You have the skeleton code and only need to write a few lines of code. What is important in this TP is not your code but your understanding of the problem. That's why we ask you to write and derive all the formulas on your report before implementing them. We will be vigilant regarding the correspondence of formulas and code.\n",
    "\n",
    "\n",
    "Here is a summary of what you will have to do :\n",
    "- implement a fully-vectorized **loss function**\n",
    "- use a validation set to **tune the learning rate and regularization** strength\n",
    "- **optimize** the loss function with **Gradient Descent** and **Newton's Method**\n",
    "- **visualize** the loss of the best models\n",
    "\n",
    "**LOOPS ARE NOT ALLOWED**. You must be able to write all the requested code for without loops, otherwise you will be penalised. \n",
    "\n",
    "**Penalty for late submission**. For each day of late submission your grade will be penalize  by -20%\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run some setup code for this notebook.\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from data_utils import load_IRIS, load_CIFAR10\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# make figures appear inline\n",
    "%matplotlib inline\n",
    "\n",
    "# notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use a dataset named CIFAR10. This dataset contains 60,000 32x32 color images in 10 different classes (CIFAR-10); open a terminal and go to the folder datasets, then execute the script get_datasets.sh:\n",
    "\n",
    "$ ./get_datasets.sh\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We will load two classes of the Cifar10 dataset. We load only 2 classes because we implement the binary logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)\n",
    "try:\n",
    "   del X_train, y_train\n",
    "   del X_test, y_test\n",
    "   print('Clear previously loaded data.')\n",
    "except:\n",
    "   pass\n",
    "\n",
    "# load 2 classes\n",
    "cifar10_dir = 'datasets/cifar-10-batches-py'\n",
    "classes=['horse', 'car']\n",
    "X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir, classes=['horse', 'car'])\n",
    "\n",
    "# Visualize some examples from the dataset.\n",
    "# We show a few examples of training images from each class.\n",
    "print(\"Visualizing some samples\")\n",
    "num_classes = len(classes)\n",
    "samples_per_class = 7\n",
    "for y, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(y_train == y)\n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace=False)\n",
    "    for i, idx in enumerate(idxs):\n",
    "        plt_idx = i * num_classes + y + 1\n",
    "        plt.subplot(samples_per_class, num_classes, plt_idx)\n",
    "        plt.imshow(X_train[idx].astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        if i == 0:\n",
    "            plt.title(cls)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# choising parameters for subsampling\n",
    "num_training = 9000\n",
    "num_validation = 1000\n",
    "\n",
    "# subsample the data\n",
    "mask = list(range(num_training, num_training + num_validation))\n",
    "X_val = X_train[mask]\n",
    "y_val = y_train[mask]\n",
    "mask = list(range(num_training))\n",
    "X_train = X_train[mask]\n",
    "y_train = y_train[mask]\n",
    "\n",
    "# Preprocessing: reshape the image data into rows\n",
    "X_train = np.reshape(X_train, (X_train.shape[0], -1))\n",
    "X_val = np.reshape(X_val, (X_val.shape[0], -1))\n",
    "X_test = np.reshape(X_test, (X_test.shape[0], -1))\n",
    "\n",
    "# Normalize the data: subtract the mean image and divide by the std\n",
    "mean_image = np.mean(X_train, axis = 0)\n",
    "std_image = np.std(X_train, axis = 0)\n",
    "X_train -= mean_image\n",
    "X_train /= std_image\n",
    "X_val -= mean_image\n",
    "X_val /= std_image\n",
    "X_test -= mean_image\n",
    "X_test /= std_image\n",
    "\n",
    "# add bias dimension and transform into columns\n",
    "X_train = np.hstack([X_train, np.ones((X_train.shape[0], 1))])\n",
    "X_val = np.hstack([X_val, np.ones((X_val.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones((X_test.shape[0], 1))])\n",
    "\n",
    "\n",
    "num_dim = X_train.shape[1]\n",
    "\n",
    "# Printing dimensions\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â First, you need to implement the logistic loss function.\n",
    "- Put the formulas on the report first !\n",
    "- Open the file logistic_regression.py and implement the scores in loss method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# scores in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=123)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "scores = logistic_regression.loss(X_train)\n",
    "print(np.sum(scores))\n",
    "if scores is None:\n",
    "    print(\"You have to implement scores first.\")\n",
    "else:\n",
    "    if  (np.abs(np.sum(scores)) - 4497.79763431513) < 1e-5:\n",
    "        print(\"Great! Your implementation of scores seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of scores seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Then implement the loss part of the loss function with an L2 regularizer! Be sure that you put the formulas on the report first ! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First implement the logistic loss function.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# loss in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=123)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "loss, _ , _= logistic_regression.loss(X_train, y_train, 0.0)\n",
    "if loss is None:\n",
    "    print(\"You have to implement loss first.\")\n",
    "else:\n",
    "    if (np.abs(loss) - 0.6933767017840157) < 1e-5:\n",
    "        print(\"Great! Your implementation of the loss seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of  the loss seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finally, implement the computation of the gradients in loss function ! Be sure to put the formulas and the derivations on the report first !\n",
    "\n",
    "- Don't forget to take into account the gradient of the regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  implement the gradients.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# grad part in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=123)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "_, grad, _ = logistic_regression.loss(X_train, y_train, 0.0)\n",
    "\n",
    "if not np.sum(grad):\n",
    "    print(\"You have to implement the gradients first.\")\n",
    "else:\n",
    "    if  (np.abs(np.sum(grad)) - 26.725907504626434) < 1e-5:\n",
    "        print(\"Great! Your implementation of gradients seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of gradients seems wrong !\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  implement the hessian.\n",
    "# Open the file logistic_regression.py and implement the\n",
    "# hessian part in loss method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=123)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "_, _, hessian = logistic_regression.loss(X_train, y_train, 0.0)\n",
    "\n",
    "print(hessian)\n",
    "print(hessian.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we need to implement the prediction method of the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open the file logistic_regression.py and implement the\n",
    "# predict() method.\n",
    "from logistic_regression import Logistic\n",
    "\n",
    "dim = X_train.shape[1]\n",
    "logistic_regression = Logistic(random_seed=123)\n",
    "logistic_regression.W = 0.001 * np.random.randn(dim, 1)\n",
    "y_pred = logistic_regression.predict(X_train)\n",
    "\n",
    "if not np.sum(y_pred):\n",
    "    print(\"You have to implement prediction first.\")\n",
    "else:\n",
    "    if (np.abs(np.sum(y_pred)) - 4718.0) < 1e-7:\n",
    "        print(\"Great! Your implementation of prediction seems good !\")\n",
    "    else:\n",
    "        print(\"Bad news! Your implementation of prediction seems wrong !\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can now use validation to tune the hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the validation set to tune hyperparameters (regularization strength and\n",
    "# learning rate). You should experiment with different ranges for the learning\n",
    "# rates and regularization strengths using Gradient Descent and newton's Method;\n",
    "from logistic_regression import Logistic\n",
    "import copy\n",
    "\n",
    "\n",
    "# to save the history of losses of best model using Gradient Descent\n",
    "best_hist_gd = []\n",
    "# to save accuracy on validation set of best model using Gradient Descent\n",
    "best_val_gd = -1\n",
    "# to save best model using Gradient Descent\n",
    "best_logistic_gd = None\n",
    "# to save the time needed for the best model using Gradient Descent\n",
    "time_best_gd = -1\n",
    "\n",
    "# to save the history of losses of best model using Newton Method\n",
    "best_hist_nw = []\n",
    "# to save accuracy on validation set of best model using Newton Method\n",
    "best_val_nw = -1\n",
    "# to save best model using Newton Method\n",
    "best_logistic_nw = None\n",
    "# to save the time needed for the best model using Newton Method\n",
    "time_best_nw = -1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "learning_rates = [1e-8, 1e-6, 1e-2]\n",
    "regularization_strengths = [1e-4, 1e-2, 0, 1e2, 1e4]\n",
    "\n",
    "# num_iters: number of iterations to train. \n",
    "# *********************************************************************************************************************#\n",
    "# For your submission you have to use num_iters=300                                                                    #\n",
    "#Â use a small value for num_iters=1 as you develop your validation code so that the model don't take much time to train#\n",
    "# Once you are sute that your validation code works, you should return the validation code                             # \n",
    "# using num_iters = 300 for your submission (the report/ comment of the results should be based on                     #\n",
    "# the best models using num_iters = 300                                                                                #\n",
    "# If using Newton's method your code is extremlly computational expensive use for Newton's method num_iters= 10         #\n",
    "# and for Gradient Descent num_iters= 300 and comment in details.                                                      #\n",
    "# *********************************************************************************************************************#\n",
    "\n",
    "\n",
    "num_iters= 2\n",
    "# if true display informations about training\n",
    "verbose = True\n",
    "newton = [False, True]\n",
    "\n",
    "for nw in newton:\n",
    "    for lr in learning_rates:\n",
    "        for reg in regularization_strengths:\n",
    "            print(\"lr = {}, reg = {}, newton = {} \".format(lr, reg, nw))\n",
    "            model = Logistic(random_seed=123)\n",
    "            ################################################################################\n",
    "            # TODO:                                                                        #\n",
    "            # Write code that chooses the best hyperparameters by tuning on the validation #\n",
    "            # set using gradient descent and newton method to update the weights.          #\n",
    "            # For both methods (Gradient descent, Newton), for each combination of         #\n",
    "            # hyperparameters, train a model on the training set, compute its accuracy on  # \n",
    "            # the training and validation sets, and store for each method                  #\n",
    "            #  1. the best validation accuracy  in best_val_gd and best_val_nw             #\n",
    "            #  2. the model object that achieves this accuracy in                          #\n",
    "            #     best_logistc_gd and best_logistc_nw.\n",
    "            # 3. the time of training that needed for the best model(time_best_gd, time_best_nt)#\n",
    "            # Plot the hisstory of losses when the best validation accuracy achieved\n",
    "            # for both gadienr descent and newton's method (best_hist_gd,best_hist_nw)     #\n",
    "            #                                                                              #\n",
    "            # Hint: You should use a small value for num_iters as you develop your         #\n",
    "            # validation code so that the model don't take much time to train; once you are#\n",
    "            # confident that your validation code works, you should rerun the validation   #\n",
    "            # code with a larger value for num_iters, lets say 200.                        #\n",
    "            #                                                                              #\n",
    "            # To copy the model use best_logistic = copy.deepcopy(model)                   #\n",
    "            ################################################################################\n",
    "\n",
    "            pass\n",
    "    \n",
    "            acc_train = 0 # to replace\n",
    "            acc_val = 0 # to replace\n",
    "        \n",
    "    \n",
    "        ################################################################################\n",
    "        #                              END OF YOUR CODE                                #\n",
    "        ################################################################################\n",
    "            print(\"\\r\\t -> train acc = {:.3f}, val acc = {:.3f}\".format(acc_train, acc_val))\n",
    "\n",
    "\n",
    "# print the best validation accuracy for gradient descent and newton's method,\n",
    "# the corresponding combination of hyperparameter, and the computational time to train them \n",
    "\n",
    "print('best validation GD: {:.3f}'.format(best_val_gd))\n",
    "print('best validation NW: {:.3f}'.format(best_val_nw))\n",
    "\n",
    "\n",
    "print(\"done \", end='\\r')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now that we have the best models, we can test the accuracy on test set ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate bast model using Gradient Descent on test set\n",
    "y_test_pred = best_logistic_gd.predict(X_test)\n",
    "test_accuracy = 0 # to replace\n",
    "print('Logistic on raw pixels final test set accuracy using Gradient Descent: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate bast model using Newton's Method on test set\n",
    "y_test_pred = best_logistic_nw.predict(X_test)\n",
    "test_accuracy = 0 # to replace\n",
    "print('Logistic on raw pixels final test set accuracy using Newtons Method: {:.3f}'.format(test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the hisstory of losses for the best models (best_hist_gd, best_hist_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
