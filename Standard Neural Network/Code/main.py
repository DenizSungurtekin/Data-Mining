# -*- coding: utf-8 -*-
"""TP5_NeuralNet.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/110IPNU4ERr8eJPjmKAmaGYQVGt7dGiSU

### TP6 NN (OBLIGATORY)
 In this TP we will develop a two-layer fully-connected neural network to perform classification, and test it out on the CIFAR-10 dataset. We train the network with a softmax loss function. The network uses a sigmoid  activation function after the first fully connected layer. The outputs of the second fully-connected layer are the scores for each class.
"""

# Commented out IPython magic to ensure Python compatibility.
import random
import numpy as np
from data_utils import load_IRIS, load_CIFAR10
from NN import NN

import matplotlib.pyplot as plt

# for auto-reloading extenrnal modules
# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython
# %load_ext autoreload
# %autoreload 2
# %matplotlib inline

plt.rcParams['figure.figsize'] = (10.0, 8.0) # set default size of plots
plt.rcParams['image.interpolation'] = 'nearest'
plt.rcParams['image.cmap'] = 'gray'

"""The CIFAR10 dataset contains 60,000 32x32 color images in 10 different classes ([CIFAR-10](https://en.wikipedia.org/wiki/CIFAR-10)) 

You have to download the dataset; open a terminal and go to the folder *datasets*, then execute the script *get_datasets.sh*:
```bash
$ ./get_datasets.sh
```
"""

def get_CIFAR10_data(num_training=5000, num_validation=500, num_test=500):
    """
    Load the CIFAR-10 dataset from disk and perform preprocessing to prepare
    it for the two-layer neural net classifier. These are the same steps as
    we used for the SVM, but condensed to a single function.  
    """
    # Load the raw CIFAR-10 data
    cifar10_dir = 'datasets/cifar-10-batches-py'
    
    X_train, y_train, X_test, y_test = load_CIFAR10(cifar10_dir)
        
    # Subsample the data
    mask = list(range(num_training, num_training + num_validation))
    X_val = X_train[mask]
    y_val = y_train[mask]
    mask = list(range(num_training))
    X_train = X_train[mask]
    y_train = y_train[mask]
    mask = list(range(num_test))
    X_test = X_test[mask]
    y_test = y_test[mask]

    # Normalize the data: subtract the mean image
    mean_image = np.mean(X_train, axis=0)
    X_train -= mean_image
    X_val -= mean_image
    X_test -= mean_image

    # Reshape data to rows
    X_train = X_train.reshape(num_training, -1)
    X_val = X_val.reshape(num_validation, -1)
    X_test = X_test.reshape(num_test, -1)

    return X_train, y_train, X_val, y_val, X_test, y_test


# Cleaning up variables to prevent loading data multiple times (which may cause memory issue)
try:
   del X_train, y_train
   del X_test, y_test
   print('Clear previously loaded data.')
except:
   pass

# Invoke the above function to get our data.
X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data()
print('Train data shape: ', X_train.shape)
print('Train labels shape: ', y_train.shape)
print('Validation data shape: ', X_val.shape)
print('Validation labels shape: ', y_val.shape)
print('Test data shape: ', X_test.shape)
print('Test labels shape: ', y_test.shape)

"""### If you use windows delete the previous cell and use (uncomment) the next one. If not delete the next cell"""

# from keras.datasets import cifar10
# from keras.utils import np_utils
# import numpy as np
# try:
#    del X_train, y_train
#    del X_test, y_test
#    print('Clear previously loaded data.')
# except:
#    pass

# # load cifar10
# (X_train, y_train), (X_test, y_test) = cifar10.load_data()

# # Reshape the image data into rows
# X_train = np.reshape(X_train, (X_train.shape[0], -1))
# X_test = np.reshape(X_test, (X_test.shape[0], -1))
# y_test = y_test.flatten()
# y_train = y_train.flatten()

# num_training = 5000
# num_validation = 500
# num_test = 500

# mask = list(range(num_training))
# X_train = X_train[mask]
# y_train = y_train[mask]
# mask = list(range(num_validation))
# X_val = X_train[mask]
# y_val = y_train[mask]
# mask = list(range(num_test))
# X_test = X_test[mask]
# y_test = y_test[mask]


# # Normalize the data: subtract the mean image and divide by the std
# mean_image = np.mean(X_train, axis = 0)
# std_image = np.std(X_train, axis = 0)

# X_train = X_train - mean_image
# X_train = X_train / std_image
# X_val = X_val - mean_image
# X_val = X_val / std_image
# X_test = X_test - mean_image
# X_test = X_test/ std_image

# # Reshape data to rows
# X_train = X_train.reshape(num_training, -1)
# X_val = X_val.reshape(num_validation, -1)
# X_test = X_test.reshape(num_test, -1)


# print('Training data shape: ', X_train.shape)
# print('Training labels shape: ', y_train.shape)
# print('Test data shape: ', X_test.shape)
# print('Test labels shape: ', y_test.shape)

"""### Instructions

We will use the class NN in the file NN.py to represent instances of our network. The network parameters are stored in the instance variable self.params where keys are string parameter names and values are numpy arrays. 

- Open the file NN.py and fill the missing parts of the NN.loss. This function takes the data and weights and computes the class scores, the loss, and the gradients on the parameters. (You have to perform the forward pass, the backward pass).

- Fill the missing part in the NN.train(). 

- Implement the NN.predict method to predict labels for data points.

- Set the number of the iteration (epochs) equal to 500 iteration and add a stopping criterion.
 - Train your network using 
     - learning rate 0.01
     - neurons in the hidden layer (hidden_size) : 1, 10, 100, 1000 
     - 1. online SGD, 2. SGD with mini batch = 200, 3. SGD with mini batch = 2000 3.mini\_batch = size of the training data set (Gradient Descent, GD)
     - for each case compute the validation accuracy and calculate the time it takes to train the network, comment.
     - Explain if and how the size of the hidden layer influences the prediction (validation  accuracy) and the computational time to train the network.
     - Explain if and how the batch  influences the prediction (validation accuracy) and the computational time to train the network.

- Find the best hyperparameters by tuning on the validation and for the best model
    - compute test accuracy
    - describe/ comment final model 
    - plot the history of the loss(train) 
    - plot the training and validation accuracy 
    - comment/ discuss
"""

input_size = 32 * 32 * 3
num_classes = 10


network = NN(input_size, hidden_size, num_classes)

#### Plot the history of the loss and the train / validatoin accuracies